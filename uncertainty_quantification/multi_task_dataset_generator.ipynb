{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ehrshot')\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from loguru import logger\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from utils import (\n",
    "    LABELING_FUNCTION_2_PAPER_NAME,\n",
    "    SHOT_STRATS,\n",
    "    MODEL_2_INFO,\n",
    "    get_labels_and_features, \n",
    "    process_chexpert_labels, \n",
    "    convert_multiclass_to_binary_labels,\n",
    "    CHEXPERT_LABELS, \n",
    "    LR_PARAMS, \n",
    "    XGB_PARAMS, \n",
    "    RF_PARAMS,\n",
    "    ProtoNetCLMBRClassifier, \n",
    "    get_patient_splits_by_idx\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from scipy.sparse import issparse\n",
    "import scipy\n",
    "import lightgbm as lgb\n",
    "import femr\n",
    "import femr.datasets\n",
    "from femr.labelers import load_labeled_patients, LabeledPatients\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, model, train_loader, val_loader, criterion, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            val_accuracy = 100 * correct / total\n",
    "            if epoch % 10 == 0:\n",
    "                pass\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            preds.extend(list(predicted.numpy()))\n",
    "            gts.extend(list(labels.numpy()))\n",
    "        test_accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "        auc_score = roc_auc_score(gts, preds)\n",
    "    return test_accuracy, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_functions=[\n",
    "    \"guo_los\",\n",
    "    \"guo_readmission\",\n",
    "    \"guo_icu\",\n",
    "    \"new_hypertension\",\n",
    "    \"new_hyperlipidemia\",\n",
    "    \"new_pancan\",\n",
    "    \"new_celiac\",\n",
    "    \"new_lupus\",\n",
    "    \"new_acutemi\",\n",
    "    \"lab_thrombocytopenia\",\n",
    "    \"lab_hyperkalemia\",\n",
    "    \"lab_hyponatremia\",\n",
    "    \"lab_anemia\",\n",
    "    \"lab_hypoglycemia\" # will OOM at 200G on `gpu` partition\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_database='../EHRSHOT_ASSETS/femr/extract'\n",
    "path_to_labels_dir='../EHRSHOT_ASSETS/benchmark'\n",
    "path_to_features_dir='../EHRSHOT_ASSETS/features'\n",
    "path_to_output_dir='../uncertainty_quantification/single_task_results'\n",
    "path_to_output_data_dir = '../uncertainty_quantification/single_task_data'\n",
    "path_to_split_csv='../EHRSHOT_ASSETS/splits/person_id_map.csv'\n",
    "path_to_data_csv = '../uncertainty_quantification'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--is_force_refresh'], dest='is_force_refresh', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='If set, then overwrite all outputs', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def parse_args() -> argparse.Namespace:\n",
    "parser = argparse.ArgumentParser(description=\"Run EHRSHOT evaluation benchmark on a specific task.\")\n",
    "parser.add_argument(\"--path_to_database\", required=True, type=str, help=\"Path to FEMR patient database\")\n",
    "parser.add_argument(\"--path_to_labels_dir\", required=True, type=str, help=\"Path to directory containing saved labels\")\n",
    "parser.add_argument(\"--path_to_features_dir\", required=True, type=str, help=\"Path to directory containing saved features\")\n",
    "parser.add_argument(\"--path_to_output_dir\", required=True, type=str, help=\"Path to directory where results will be saved\")\n",
    "parser.add_argument(\"--path_to_split_csv\", required=True, type=str, help=\"Path to CSV of splits\")\n",
    "parser.add_argument(\"--labeling_function\", required=True, type=str, help=\"Labeling function for which we will create k-shot samples.\", choices=LABELING_FUNCTION_2_PAPER_NAME.keys(), )\n",
    "parser.add_argument(\"--num_threads\", type=int, help=\"Number of threads to use\")\n",
    "parser.add_argument(\"--is_force_refresh\", action='store_true', default=False, help=\"If set, then overwrite all outputs\")\n",
    "# return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = pd.DataFrame()\n",
    "X_val_all = pd.DataFrame()\n",
    "X_test_all = pd.DataFrame()\n",
    "\n",
    "y_train_all = pd.DataFrame()\n",
    "y_val_all = pd.DataFrame()\n",
    "y_test_all = pd.DataFrame()\n",
    "\n",
    "task_indicator = []\n",
    "# for i in tqdm(range(len(labeling_functions))):\n",
    "for i in range(len(labeling_functions)):\n",
    "    labeling_function = labeling_functions[i]\n",
    "    args = parser.parse_args(f'--labeling_function {labeling_function} --path_to_database {path_to_database} --path_to_labels_dir {path_to_labels_dir} --path_to_features_dir {path_to_features_dir} --path_to_split_csv {path_to_split_csv} --path_to_output_dir {path_to_output_dir}'.split())\n",
    "\n",
    "    LABELING_FUNCTION: str = args.labeling_function\n",
    "    # SHOT_STRAT: str = args.shot_strat\n",
    "    # NUM_THREADS: int = args.num_threads\n",
    "    IS_FORCE_REFRESH: bool = args.is_force_refresh\n",
    "    PATH_TO_DATABASE: str = args.path_to_database\n",
    "    PATH_TO_FEATURES_DIR: str = args.path_to_features_dir\n",
    "    PATH_TO_LABELS_DIR: str = args.path_to_labels_dir\n",
    "    PATH_TO_SPLIT_CSV: str = args.path_to_split_csv\n",
    "    PATH_TO_LABELED_PATIENTS: str = os.path.join(PATH_TO_LABELS_DIR, LABELING_FUNCTION, 'labeled_patients.csv')\n",
    "    PATH_TO_OUTPUT_DIR: str = args.path_to_output_dir\n",
    "    PATH_TO_OUTPUT_FILE: str = os.path.join(PATH_TO_OUTPUT_DIR, f'{LABELING_FUNCTION}_results.csv')\n",
    "    # PATH_TO_OUTPUT_DATA: str = \n",
    "\n",
    "    database = femr.datasets.PatientDatabase('../EHRSHOT_ASSETS/femr/extract')\n",
    "\n",
    "    labeled_patients: LabeledPatients = load_labeled_patients(PATH_TO_LABELED_PATIENTS)\n",
    "    patient_ids, label_values, label_times, feature_matrixes = get_labels_and_features(labeled_patients, PATH_TO_FEATURES_DIR)\n",
    "    train_pids_idx, val_pids_idx, test_pids_idx = get_patient_splits_by_idx(PATH_TO_SPLIT_CSV, patient_ids)\n",
    "\n",
    "    if LABELING_FUNCTION == \"chexpert\":\n",
    "        label_values = process_chexpert_labels(label_values)\n",
    "        sub_tasks: List[str] = CHEXPERT_LABELS\n",
    "    elif LABELING_FUNCTION.startswith('lab_'):\n",
    "        # Lab value is multi-class, convert to binary\n",
    "        label_values = convert_multiclass_to_binary_labels(label_values, threshold=1)\n",
    "        sub_tasks: List[str] = [LABELING_FUNCTION]\n",
    "    else:\n",
    "        # Binary classification\n",
    "        sub_tasks: List[str] = [LABELING_FUNCTION]\n",
    "            \n",
    "\n",
    "    model = 'clmbr'\n",
    "    X_train: np.ndarray = feature_matrixes[model][train_pids_idx]\n",
    "    X_val: np.ndarray = feature_matrixes[model][val_pids_idx]\n",
    "    X_test: np.ndarray = feature_matrixes[model][test_pids_idx]\n",
    "\n",
    "    y_train: np.array = label_values[train_pids_idx].astype(int)\n",
    "    y_val: np.array = label_values[val_pids_idx].astype(int)\n",
    "    y_test: np.ndarray = label_values[test_pids_idx].astype(int)\n",
    "    \n",
    "\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_val = pd.DataFrame(X_val)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    X_train['task'] = labeling_function\n",
    "    X_val['task'] = labeling_function\n",
    "    X_test['task'] = labeling_function\n",
    "\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_val = pd.DataFrame(y_val)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    X_train_all = pd.concat([X_train_all, X_train])\n",
    "    X_val_all = pd.concat([X_val_all, X_val])\n",
    "    X_test_all = pd.concat([X_test_all, X_test])\n",
    "\n",
    "    y_train_all = pd.concat([y_train_all, y_train])\n",
    "    y_val_all = pd.concat([y_val_all, y_val])\n",
    "    y_test_all = pd.concat([y_test_all, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_all.to_csv(os.path.join(path_to_data_csv, 'multi_task_data', 'X_train_all.csv'))\n",
    "# y_train_all.to_csv(os.path.join(path_to_data_csv, 'multi_task_data', 'y_train_all.csv'))\n",
    "# X_val_all.to_csv(os.path.join(path_to_data_csv, 'multi_task_data', 'X_val_all.csv'))\n",
    "# y_val_all.to_csv(os.path.join(path_to_data_csv, 'multi_task_data', 'y_val_all.csv'))\n",
    "# X_test_all.to_csv(os.path.join(path_to_data_csv, 'multi_task_data', 'X_test_all.csv'))\n",
    "# y_test_all.to_csv(os.path.join(path_to_data_csv, 'multi_task_data', 'y_test_all.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122103</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122104</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122105</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122107</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439895 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "...    ..\n",
       "122103  0\n",
       "122104  0\n",
       "122105  0\n",
       "122106  0\n",
       "122107  0\n",
       "\n",
       "[439895 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EHRSHOT_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
