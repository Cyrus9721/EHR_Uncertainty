{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ehrshot')\n",
    "import copy\n",
    "from typing import Literal\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Distribution\n",
    "from torch_uncertainty.utils.distributions import cat_dist\n",
    "from torch_uncertainty.routines import ClassificationRoutine\n",
    "from torch_uncertainty.utils import TUTrainer\n",
    "from torch_uncertainty.models import deep_ensembles, mc_dropout\n",
    "from torch_uncertainty.transforms import RepeatTarget\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "labeling_functions=[\n",
    "    \"guo_los\",\n",
    "    \"guo_readmission\",\n",
    "    \"guo_icu\",\n",
    "    \"new_hypertension\",\n",
    "    \"new_hyperlipidemia\",\n",
    "    \"new_pancan\",\n",
    "    \"new_celiac\",\n",
    "    \"new_lupus\",\n",
    "    \"new_acutemi\",\n",
    "    \"lab_thrombocytopenia\",\n",
    "    \"lab_hyperkalemia\",\n",
    "    \"lab_hyponatremia\",\n",
    "    \"lab_anemia\",\n",
    "    \"lab_hypoglycemia\" # will OOM at 200G on `gpu` partition\n",
    "]\n",
    "\n",
    "unique_tasks_1 = ['guo_los', 'guo_readmission', 'guo_icu']\n",
    "unique_tasks_2 = ['new_hypertension', 'new_hyperlipidemia', 'new_pancan', 'new_celiac', 'new_lupus', 'new_acutemi']\n",
    "unique_tasks_3 = ['lab_thrombocytopenia', 'lab_hyperkalemia', 'lab_hyponatremia', 'lab_anemia', 'lab_hypoglycemia']\n",
    "\n",
    "all_tasks = [unique_tasks_1, unique_tasks_2, unique_tasks_3]\n",
    "all_tasks_name = ['unique_tasks_1', 'unique_tasks_2', 'unique_tasks_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def optim_recipe(model, lr_mult: float = 1.0):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.05 * lr_mult)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    return {\"optimizer\": optimizer, \"scheduler\": scheduler}\n",
    "\n",
    "max_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# for i in tqdm(range(len(all_tasks))):\n",
    "for i in [0]:\n",
    "\n",
    "    general_task_name = all_tasks_name[i]\n",
    "\n",
    "    folder_path = f'multi_task_data_uq/{general_task_name}'\n",
    "\n",
    "    train_x_name = os.path.join(folder_path, 'X_train_all.csv')\n",
    "    train_y_name = os.path.join(folder_path, 'y_train_all.csv')\n",
    "    val_x_name = os.path.join(folder_path, 'X_val_all.csv')\n",
    "    val_y_name = os.path.join(folder_path, 'y_val_all.csv')\n",
    "\n",
    "    X_train = pd.read_csv(train_x_name).to_numpy()\n",
    "    y_train = pd.read_csv(train_y_name).to_numpy().reshape(-1)\n",
    "    X_val = pd.read_csv(val_x_name).to_numpy()\n",
    "    y_val = pd.read_csv(val_y_name).to_numpy().reshape(-1)\n",
    "\n",
    "    # Create class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    X_val = torch.tensor(X_val).float()\n",
    "\n",
    "    y_train = torch.tensor(y_train).long()\n",
    "    y_val = torch.tensor(y_val).long()\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 128\n",
    "    num_classes = 2\n",
    "    model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "    ensemble = deep_ensembles(\n",
    "        model,\n",
    "        num_estimators=5,\n",
    "        task=\"classification\",\n",
    "        reset_model_parameters=True,\n",
    "    )\n",
    "\n",
    "    trainer = TUTrainer(accelerator=\"gpu\", max_epochs=max_epochs)\n",
    "\n",
    "    ens_routine = ClassificationRoutine(\n",
    "        is_ensemble=True,\n",
    "        num_classes=2,\n",
    "        model=ensemble,\n",
    "        loss=nn.CrossEntropyLoss(weight=class_weights),\n",
    "        format_batch_fn=RepeatTarget(\n",
    "            5\n",
    "        ), \n",
    "        optim_recipe=optim_recipe(\n",
    "            ensemble, 2.0\n",
    "        ),\n",
    "        eval_ood=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(ens_routine, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "        ens_perf = trainer.test(ens_routine, dataloaders=[test_dl])\n",
    "\n",
    "        results_dict[current_all_tasks_list[j]] = ens_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('results_model_uq/results_multi_task_deep_ensemble.json', 'w') as f:\n",
    "#     json.dump(results_dict, f)\n",
    "\n",
    "import json\n",
    "with open('results_model_uq_v2/results_single_task_deep_ensemble.json', 'w') as f:\n",
    "    json.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
