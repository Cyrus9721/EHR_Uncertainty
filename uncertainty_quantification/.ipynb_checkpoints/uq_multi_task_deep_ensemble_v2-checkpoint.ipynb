{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ehrshot')\n",
    "import copy\n",
    "from typing import Literal\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Distribution\n",
    "from torch_uncertainty.utils.distributions import cat_dist\n",
    "from torch_uncertainty.routines import ClassificationRoutine\n",
    "from torch_uncertainty.utils import TUTrainer\n",
    "from torch_uncertainty.models import deep_ensembles, mc_dropout\n",
    "from torch_uncertainty.transforms import RepeatTarget\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_uncertainty.metrics.classification.brier_score as brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch_uncertainty.metrics.classification.brier_score' from '/home/zizhang/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/torch_uncertainty/metrics/classification/brier_score.py'>\n"
     ]
    }
   ],
   "source": [
    "print(brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "\n",
    "unique_tasks_1 = ['value_los', 'value_icu']\n",
    "unique_tasks_2 = ['value_hypoglycemia', 'value_hyperkalemia', 'value_hyponatremia', 'value_anemia', 'value_thrombocytopenia']\n",
    "unique_tasks_3 = ['value_new_hypertension', 'value_new_hyperlipidemia', 'value_new_acutemi']\n",
    "\n",
    "\n",
    "all_tasks = [unique_tasks_1, unique_tasks_2, unique_tasks_3]\n",
    "all_tasks_name = ['general_operation_v1', 'lab_test', 'new_diagnose']\n",
    "embed_df = pd.read_csv('embedding_matrix/embed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_task = unique_tasks_1 + unique_tasks_2 + unique_tasks_3\n",
    "# def generate_embeddings(tasks, dimensions, mean, std_dev):\n",
    "#     np.random.seed(42)\n",
    "#     embeddings = {}\n",
    "\n",
    "#     for task in tasks:\n",
    "#         embeddings[task] = np.random.normal(mean, std_dev, dimensions)\n",
    "\n",
    "#     # Convert the dictionary of embeddings to a DataFrame\n",
    "#     df_embeddings = pd.DataFrame(embeddings)\n",
    "\n",
    "#     return df_embeddings\n",
    "\n",
    "# df = generate_embeddings(embed_task, 768, 0, 0.5)\n",
    "# df.to_csv('embedding_matrix/embed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.dropout2 = nn.Dropout(p=0.2)\n",
    "#         self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def optim_recipe(model, lr_mult: float = 1.0):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01 * lr_mult, momentum=0.9)\n",
    "    return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  3.12it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.09it/s]\u001b[A\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                | Type             | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | model               | _DeepEnsembles   | 493 K  | train\n",
      "1 | loss                | CrossEntropyLoss | 0      | train\n",
      "2 | format_batch_fn     | RepeatTarget     | 0      | train\n",
      "3 | val_cls_metrics     | MetricCollection | 0      | train\n",
      "4 | test_cls_metrics    | MetricCollection | 0      | train\n",
      "5 | test_id_entropy     | Entropy          | 0      | train\n",
      "6 | test_id_ens_metrics | MetricCollection | 0      | train\n",
      "7 | mixup               | Identity         | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "493 K     Trainable params\n",
      "0         Non-trainable params\n",
      "493 K     Total params\n",
      "1.974     Total estimated model params size (MB)\n",
      "50        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zizhang/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2]) torch.Size([128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zizhang/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1d1bb42e2947d49e3649fced6a0bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4104, 2]) torch.Size([4104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[ALOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/zizhang/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b193c7a27e49978adb14b752098daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2037, 2]) torch.Size([2037])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          23.56%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.52896          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.64004          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.91814          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.42115          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.42115          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.37715          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.74704          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           nan%            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          75.77%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         23.56%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.52896         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.64004         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.91814         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.42115         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.42115         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.37715         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.74704         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          nan%           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         75.77%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|██████████████████████▌                      | 1/2 [00:00<00:00,  2.42it/s]\u001b[ALOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/zizhang/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15567ddd6a04e1ab5421b5052de4008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2037, 2]) torch.Size([2037])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">      Classification       </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     Acc      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          95.83%           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    Brier     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.51512          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   Entropy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.55180          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     NLL      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.32681          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\">        Calibration        </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     ECE      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.20140          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     aECE     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.20140          </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Test metric  </span>┃<span style=\"font-weight: bold\"> Selective Classification  </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    AUGRC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.02027          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     AURC     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.04247          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Cov@5Risk   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           nan%            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Risk@80Cov  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           3.87%           </span>│\n",
       "└──────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Classification      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    Acc     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         95.83%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   Brier    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.51512         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  Entropy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.55180         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    NLL     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.32681         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Calibration       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    ECE     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.20140         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    aECE    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.20140         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTest metric \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSelective Classification \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   AUGRC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.02027         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    AURC    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.04247         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Cov@5Risk  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          nan%           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Risk@80Cov \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          3.87%          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.45it/s]\u001b[A\n",
      " 33%|███████████████                              | 1/3 [00:02<00:05,  2.66s/it]\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|█████████                                    | 1/5 [00:06<00:27,  6.94s/it]\u001b[A\n",
      " 33%|███████████████                              | 1/3 [00:09<00:19,  9.60s/it]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m specific_task_name \u001b[38;5;241m=\u001b[39m all_tasks[i][j]\n\u001b[1;32m     26\u001b[0m specific_task_embed \u001b[38;5;241m=\u001b[39m embed_df[specific_task_name]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 28\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     29\u001b[0m X_val \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(val_x_name)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     31\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train \u001b[38;5;241m+\u001b[39m specific_task_embed\n",
      "File \u001b[0;32m~/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/uq_ehr/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(len(all_tasks))):\n",
    "# for i in [0]:\n",
    "    general_task_name = all_tasks_name[i]\n",
    "\n",
    "    folder_path = f'same_time_data/{general_task_name}'\n",
    "    \n",
    "    train_x_name = os.path.join(folder_path, 'x_train.csv')\n",
    "    train_y_name = os.path.join(folder_path, 'y_train.csv')\n",
    "    val_x_name = os.path.join(folder_path, 'x_val.csv')\n",
    "    val_y_name = os.path.join(folder_path, 'y_val.csv')\n",
    "    test_x_name = os.path.join(folder_path, 'x_test.csv')\n",
    "    test_y_name = os.path.join(folder_path, 'y_test.csv')\n",
    "\n",
    "    X_train_all = []\n",
    "    X_val_all = []\n",
    "    \n",
    "    y_train_all = []\n",
    "    y_val_all = []\n",
    "    \n",
    "    for j in tqdm(range(len(all_tasks[i]))):\n",
    "    # for j in range(1):\n",
    "        specific_task_name = all_tasks[i][j]\n",
    "        specific_task_embed = embed_df[specific_task_name].values\n",
    "        \n",
    "        X_train = pd.read_csv(train_x_name).to_numpy()\n",
    "        X_val = pd.read_csv(val_x_name).to_numpy()\n",
    "        \n",
    "        X_train = X_train + specific_task_embed\n",
    "        X_val = X_val + specific_task_embed\n",
    "        \n",
    "        y_train = pd.read_csv(train_y_name)[specific_task_name].astype(int).to_numpy()\n",
    "        y_val = pd.read_csv(val_y_name)[specific_task_name].astype(int).to_numpy()\n",
    "        \n",
    "        X_train_all.append(X_train)\n",
    "        X_val_all.append(X_val)\n",
    "        \n",
    "        y_train_all.append(y_train)\n",
    "        y_val_all.append(y_val)\n",
    "        \n",
    "    X_train_all = np.concatenate(X_train_all, axis = 0)\n",
    "    X_val_all = np.concatenate(X_val_all, axis = 0)\n",
    "    \n",
    "    y_train_all = np.concatenate(y_train_all)\n",
    "    y_val_all = np.concatenate(y_val_all)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_all), y=y_train_all)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "    \n",
    "    X_train_all = torch.tensor(X_train_all).float()\n",
    "    X_val_all = torch.tensor(X_val_all).float()\n",
    "    \n",
    "    y_train_all = torch.tensor(y_train_all).long()\n",
    "    y_val_all = torch.tensor(y_val_all).long()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_all, y_train_all)\n",
    "    val_dataset = TensorDataset(X_val_all, y_val_all)\n",
    "    \n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    ###################################################################################################\n",
    "    \n",
    "    \n",
    "    input_size = X_train_all.shape[1]\n",
    "    hidden_size = 128\n",
    "    num_classes = 2\n",
    "    model = NN(input_size, hidden_size, num_classes)\n",
    "    \n",
    "    ensemble = deep_ensembles(\n",
    "        model,\n",
    "        num_estimators=5,\n",
    "        task=\"classification\",\n",
    "        reset_model_parameters=True,\n",
    "    )\n",
    "    \n",
    "    trainer = TUTrainer(accelerator=\"gpu\", max_epochs=max_epochs)\n",
    "\n",
    "    ens_routine = ClassificationRoutine(\n",
    "        is_ensemble=True,\n",
    "        num_classes=2,\n",
    "        model=ensemble,\n",
    "        loss=nn.CrossEntropyLoss(weight=class_weights),\n",
    "        format_batch_fn=RepeatTarget(\n",
    "            5\n",
    "        ), \n",
    "        optim_recipe=optim_recipe(\n",
    "            ensemble, 1\n",
    "        ),\n",
    "        eval_ood=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(ens_routine, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    for k in tqdm(range(len(all_tasks[i]))):\n",
    "        specific_task_name_test = all_tasks[i][k]\n",
    "        specific_task_embed_k = embed_df[specific_task_name_test].values\n",
    "        \n",
    "        X_test = pd.read_csv(test_x_name).to_numpy()\n",
    "        X_test = X_test + specific_task_embed_k\n",
    "        y_test = pd.read_csv(test_y_name)[specific_task_name_test].astype(int).to_numpy()\n",
    "        \n",
    "        X_test = torch.tensor(X_test).float()\n",
    "        y_test = torch.tensor(y_test).long()\n",
    "        \n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "        test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        ens_perf = trainer.test(ens_routine, dataloaders=[test_dl])\n",
    "\n",
    "        results_dict[specific_task_name_test] = ens_perf\n",
    "        \n",
    "    del trainer, ens_routine, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('results_model_uq_v2/results_multi_task_deep_ensemble.json', 'w') as f:\n",
    "#     json.dump(results_dict, f)\n",
    "\n",
    "\n",
    "import json\n",
    "with open('results_model_uq_v3/results_multi_task_deep_ensemble.json', 'w') as f:\n",
    "    json.dump(results_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['value_los', 'value_icu', 'value_hypoglycemia', 'value_hyperkalemia', 'value_hyponatremia', 'value_anemia', 'value_thrombocytopenia', 'value_new_hypertension', 'value_new_hyperlipidemia', 'value_new_acutemi'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_epochs = 50\n",
    "# batch_size = 64\n",
    "\n",
    "# # for i in tqdm(range(len(all_tasks))):\n",
    "# for i in [0]:\n",
    "#     general_task_name = all_tasks_name[i]\n",
    "\n",
    "#     folder_path = f'same_time_data/{general_task_name}'\n",
    "    \n",
    "#     train_x_name = os.path.join(folder_path, 'x_train.csv')\n",
    "#     train_y_name = os.path.join(folder_path, 'y_train.csv')\n",
    "#     val_x_name = os.path.join(folder_path, 'x_val.csv')\n",
    "#     val_y_name = os.path.join(folder_path, 'y_val.csv')\n",
    "#     test_x_name = os.path.join(folder_path, 'x_test.csv')\n",
    "#     test_y_name = os.path.join(folder_path, 'y_test.csv')\n",
    "\n",
    "#     X_train = pd.read_csv(train_x_name).to_numpy()\n",
    "#     X_val = pd.read_csv(val_x_name).to_numpy()\n",
    "#     X_test = pd.read_csv(test_x_name).to_numpy()\n",
    "\n",
    "#     X_all = pd.DataFrame()\n",
    "#     X_val = pd.DataFrame()\n",
    "#     X_test = \n",
    "    \n",
    "#     for j in tqdm(range(len(all_tasks[i]))):\n",
    "#     # for j in range(1):\n",
    "#         specific_task_name = all_tasks[i][j]\n",
    "#         y_train = pd.read_csv(train_y_name)[specific_task_name].astype(int).to_numpy()\n",
    "#         y_val = pd.read_csv(val_y_name)[specific_task_name].astype(int).to_numpy()\n",
    "#         y_test = pd.read_csv(test_y_name)[specific_task_name].astype(int).to_numpy()\n",
    "\n",
    "#         assert len(np.unique(y_train)) == 2\n",
    "#         # Create class weights\n",
    "#         class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "#         class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "#         X_train = torch.tensor(X_train).float()\n",
    "#         X_val = torch.tensor(X_val).float()\n",
    "#         X_test = torch.tensor(X_test).float()\n",
    "\n",
    "#         y_train = torch.tensor(y_train).long()\n",
    "#         y_val = torch.tensor(y_val).long()\n",
    "#         y_test = torch.tensor(y_test).long()\n",
    "\n",
    "#         # Create TensorDatasets\n",
    "#         train_dataset = TensorDataset(X_train, y_train)\n",
    "#         val_dataset = TensorDataset(X_val, y_val)\n",
    "#         test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "#         # Create DataLoaders\n",
    "#         train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#         test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         input_size = X_train.shape[1]\n",
    "#         hidden_size = 128\n",
    "#         num_classes = 2\n",
    "#         model = NN(input_size, hidden_size, num_classes)\n",
    "\n",
    "#         trainer = TUTrainer(accelerator=\"gpu\", max_epochs=max_epochs)\n",
    "\n",
    "#         ens_routine = ClassificationRoutine(\n",
    "#             is_ensemble=True,\n",
    "#             num_classes=2,\n",
    "#             model=model,\n",
    "#             loss=nn.CrossEntropyLoss(weight=class_weights),\n",
    "#             format_batch_fn=RepeatTarget(\n",
    "#                 1\n",
    "#             ), \n",
    "#             optim_recipe=optim_recipe(\n",
    "#                 model, 1\n",
    "#             ),\n",
    "#             eval_ood=False,\n",
    "#         )\n",
    "\n",
    "#         trainer.fit(ens_routine, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "\n",
    "#         ens_perf = trainer.test(ens_routine, dataloaders=[test_dl])\n",
    "\n",
    "#         results_dict[specific_task_name] = ens_perf\n",
    "        \n",
    "#         del trainer, ens_routine, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.numpy().std(), X_train.numpy().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uq_ehr]",
   "language": "python",
   "name": "conda-env-uq_ehr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
